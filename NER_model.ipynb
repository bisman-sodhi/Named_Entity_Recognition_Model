{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:00.449347Z",
          "iopub.status.busy": "2022-02-17T08:25:00.449032Z",
          "iopub.status.idle": "2022-02-17T08:25:00.482131Z",
          "shell.execute_reply": "2022-02-17T08:25:00.481441Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.449262Z"
        },
        "id": "SWWjBbIZ5LSi",
        "outputId": "af701961-2dae-45b1-820c-0e0e0030495e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/dist-packages (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (23.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (0.11.4)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (0.8.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2023.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, List, Optional\n",
        "from collections import Counter\n",
        "import os\n",
        "import csv\n",
        "!pip install torchmetrics\n",
        "!pip install pytorch-metric-learning\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "!pip install pytorch-lightning\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:00.483925Z",
          "iopub.status.busy": "2022-02-17T08:25:00.483609Z",
          "iopub.status.idle": "2022-02-17T08:25:00.494599Z",
          "shell.execute_reply": "2022-02-17T08:25:00.493679Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.483881Z"
        },
        "id": "u29mNAdI5LSl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        # two special tokens for padding and unknown\n",
        "        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        self.idx2token = [\"<pad>\", \"<unk>\"]\n",
        "        self.is_fit = False\n",
        "    \n",
        "    @property\n",
        "    def pad_id(self):\n",
        "        return self.token2idx[\"<pad>\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "    \n",
        "    def fit(self, train_texts: List[str]):\n",
        "        counter = Counter()\n",
        "        for text in train_texts:\n",
        "            counter.update(text.lower().split())\n",
        "        \n",
        "        # manually set a vocabulary size for the data set\n",
        "        vocab_size = 20000\n",
        "        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n",
        "        for (i, token) in enumerate(self.idx2token):\n",
        "            self.token2idx[token] = i\n",
        "            \n",
        "        self.is_fit = True\n",
        "                \n",
        "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
        "        if not self.is_fit:\n",
        "            raise Exception(\"Please fit the tokenizer on the training tokens\")\n",
        "        \n",
        "        text_ids = text.strip().split()\n",
        "        token_ids_num = []\n",
        "        char_count = 1\n",
        "        \n",
        "        for every_word in text_ids:  \n",
        "          every_word = every_word.lower()        \n",
        "          if char_count > max_length and max_length != None:\n",
        "            break\n",
        "          char_count += 1 \n",
        "\n",
        "          if every_word in self.token2idx:\n",
        "            token_ids_num.append(self.token2idx[every_word])\n",
        "          else:\n",
        "            token_ids_num.append(self.token2idx['<unk>'])\n",
        "        \n",
        "        if max_length is None:\n",
        "            return token_ids_num\n",
        "        # truncate the tags if longer than max_length\n",
        "        if len(token_ids_num) > max_length:\n",
        "            return token_ids_num[:max_length]\n",
        "        # pad with 0s if shorter than max_length\n",
        "        if  max_length != None and char_count <= max_length:\n",
        "            return token_ids_num + [self.token2idx['<pad>']] * (max_length - len(token_ids_num))  # 0 as padding for tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:00.496201Z",
          "iopub.status.busy": "2022-02-17T08:25:00.495804Z",
          "iopub.status.idle": "2022-02-17T08:25:00.504688Z",
          "shell.execute_reply": "2022-02-17T08:25:00.503898Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.496166Z"
        },
        "id": "7lHbdxRn5LSm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_raw_data(filepath: str, with_tags: bool = True):\n",
        "    data = {'text': []}\n",
        "    if with_tags:\n",
        "        data['tags'] = []\n",
        "        with open(filepath) as f:\n",
        "            reader = csv.reader(f)\n",
        "            for text, tags in reader:\n",
        "                data['text'].append(text)\n",
        "                data['tags'].append(tags)\n",
        "    else:\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                data['text'].append(line.strip())\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:00.518378Z",
          "iopub.status.busy": "2022-02-17T08:25:00.517671Z",
          "iopub.status.idle": "2022-02-17T08:25:00.795602Z",
          "shell.execute_reply": "2022-02-17T08:25:00.794913Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.518340Z"
        },
        "id": "dEuoJh1Q5LSn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "data_dir = \"/content/\"\n",
        "train_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\n",
        "val_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\n",
        "test_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n",
        "# fit the tokenizer on the training tokens\n",
        "tokenizer.fit(train_raw['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "jertEpiNlqXM"
      },
      "outputs": [],
      "source": [
        "#modify as per workspace\n",
        "tokenizer = Tokenizer()\n",
        "train_raw = load_raw_data(os.path.join(\"train.csv\"))\n",
        "val_raw = load_raw_data(os.path.join(\"val.csv\"))\n",
        "test_raw = load_raw_data(os.path.join(\"test_tokens.txt\"), with_tags=False)\n",
        "# fit the tokenizer on the training tokens\n",
        "tokenizer.fit(train_raw['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "KzUsGMealyZb"
      },
      "outputs": [],
      "source": [
        "class NERDataset: \n",
        "    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n",
        "    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "  \n",
        "    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_ids = []\n",
        "        self.tag_ids = []\n",
        "        self.with_tags = False\n",
        "        for text in raw_data['text']:\n",
        "            self.token_ids.append(tokenizer.encode(text, max_length=max_length))\n",
        "        if 'tags' in raw_data:\n",
        "            self.with_tags = True\n",
        "            for tags in raw_data['tags']:\n",
        "                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n",
        "    \n",
        "    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n",
        "        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n",
        "        if max_length is None:\n",
        "            return tag_ids\n",
        "        # truncate the tags if longer than max_length\n",
        "        if len(tag_ids) > max_length:\n",
        "            return tag_ids[:max_length]\n",
        "        # pad with 0s if shorter than max_length\n",
        "        else:\n",
        "            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = torch.LongTensor(self.token_ids[idx])\n",
        "        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n",
        "        if self.with_tags:\n",
        "            # for training and validation\n",
        "            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n",
        "        else:\n",
        "            # for testing\n",
        "            return token_ids, mask\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:02.109921Z",
          "iopub.status.busy": "2022-02-17T08:25:02.109558Z",
          "iopub.status.idle": "2022-02-17T08:25:02.467151Z",
          "shell.execute_reply": "2022-02-17T08:25:02.466435Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.109883Z"
        },
        "id": "0kMIKu-p5LSo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tr_data = NERDataset(train_raw, tokenizer)\n",
        "va_data = NERDataset(val_raw, tokenizer)\n",
        "te_data = NERDataset(test_raw, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "LUJ5gekGC1MO"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "    super ().__init__()\n",
        "    self.dropout = nn.Dropout (p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "    )\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "  def forward (self, x: torch. Tensor) -> torch. Tensor:\n",
        "    x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "    return self.dropout (x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:02.468821Z",
          "iopub.status.busy": "2022-02-17T08:25:02.468580Z",
          "iopub.status.idle": "2022-02-17T08:25:02.478006Z",
          "shell.execute_reply": "2022-02-17T08:25:02.477246Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.468786Z"
        },
        "id": "Jvy2pBQL5LSo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
        "        self.d_model = embed_size\n",
        "        self.decoder = nn.Linear(embed_size, 10)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        \n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src.transpose(1,0), src_key_padding_mask = src_mask)\n",
        "        output = self.decoder(output.transpose(1,0))\n",
        "        return output  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:03.082694Z",
          "iopub.status.busy": "2022-02-17T08:25:03.082423Z",
          "iopub.status.idle": "2022-02-17T08:25:03.091306Z",
          "shell.execute_reply": "2022-02-17T08:25:03.090373Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.082660Z"
        },
        "id": "Y5Eaibzu5LSp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def validate(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    device: torch.device,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            # output shape: (batch_size, max_length, num_classes)\n",
        "            logits = model(input_ids, input_mask)\n",
        "            # ignore padding index 0 when calculating loss\n",
        "            loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "                \n",
        "            loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "            is_active = torch.logical_not(input_mask)  # non-padding elements\n",
        "            # only consider non-padded tokens when calculating accuracy\n",
        "            acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    print(f\"| Validate | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:03.093232Z",
          "iopub.status.busy": "2022-02-17T08:25:03.092754Z",
          "iopub.status.idle": "2022-02-17T08:25:03.104319Z",
          "shell.execute_reply": "2022-02-17T08:25:03.103543Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.093195Z"
        },
        "id": "qQtTOXRA5LSp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.train()\n",
        "    \n",
        "    # loop through all batches in the training\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # output shape: (batch_size, max_length, num_classes)\n",
        "        logits = model(input_ids, input_mask)\n",
        "        # ignore padding index 0 when calculating loss\n",
        "        loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "        is_active = torch.logical_not(input_mask)  # non-padding elements\n",
        "        # only consider non-padded tokens when calculating accuracy\n",
        "        acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    print(f\"| Epoch {epoch} | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:03.105982Z",
          "iopub.status.busy": "2022-02-17T08:25:03.105602Z",
          "iopub.status.idle": "2022-02-17T08:25:43.205981Z",
          "shell.execute_reply": "2022-02-17T08:25:43.205084Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.105946Z"
        },
        "id": "5Be4ZCs15LSq",
        "outputId": "21bcd433-a93f-4093-ce09-74c4ed21a0c5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 439/439 [05:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch 0 | loss 0.3279 | acc 0.9115 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 439/439 [05:06<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch 1 | loss 0.1050 | acc 0.9683 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 439/439 [05:06<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch 2 | loss 0.0591 | acc 0.9815 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 439/439 [05:07<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch 3 | loss 0.0431 | acc 0.9862 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 439/439 [05:07<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch 4 | loss 0.0388 | acc 0.9875 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 102/102 [00:21<00:00,  4.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Validate | loss 0.2947 | acc 0.9426 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# data loaders\n",
        "train_dataloader = DataLoader(tr_data, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(va_data, batch_size=32)\n",
        "test_dataloader = DataLoader(te_data, batch_size=32)\n",
        "\n",
        "# move the model to device\n",
        "model = TransformerModel(vocab_size = len(tokenizer), \n",
        "    embed_size = 256, \n",
        "    num_heads = 4, \n",
        "    hidden_size = 256,\n",
        "    num_layers = 2,).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(model, train_dataloader, optimizer, device, epoch)\n",
        "validate(model, val_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:43.207812Z",
          "iopub.status.busy": "2022-02-17T08:25:43.207551Z",
          "iopub.status.idle": "2022-02-17T08:25:43.426052Z",
          "shell.execute_reply": "2022-02-17T08:25:43.425331Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.207774Z"
        },
        "id": "2BeTuu4i5LSq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO: implement the predict function\n",
        "from cmath import inf\n",
        "\n",
        "def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(dataloader):\n",
        "        input_ids, input_mask = batch[0].to(device), batch[1].to(device)\n",
        "        logits = model(input_ids, input_mask)\n",
        "        input_ids = input_ids.tolist()\n",
        "        prec = torch.argmax(logits, dim = 2).tolist()\n",
        "        sentence_index = 0        \n",
        "        while sentence_index < len(prec):\n",
        "          counter = 0\n",
        "          inner_list = []\n",
        "          for word in prec[sentence_index]:\n",
        "            if input_ids[sentence_index][counter] == 0:\n",
        "              break\n",
        "            counter += 1\n",
        "            inner_list.append(tr_data.idx2tag[word])\n",
        "          preds.append(inner_list)\n",
        "          sentence_index+=1\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:43.431127Z",
          "iopub.status.busy": "2022-02-17T08:25:43.430210Z",
          "iopub.status.idle": "2022-02-17T08:25:44.519771Z",
          "shell.execute_reply": "2022-02-17T08:25:44.519005Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.431084Z"
        },
        "id": "EzFjEe0c5LSq",
        "outputId": "e58509be-e611-43bc-dfc3-44276b793246",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-16 04:28:47--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py.11’\n",
            "\n",
            "\rconlleval.py.11       0%[                    ]       0  --.-KB/s               \rconlleval.py.11     100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-16 04:28:47 (86.9 MB/s) - ‘conlleval.py.11’ saved [7502/7502]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "from conlleval import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:44.521511Z",
          "iopub.status.busy": "2022-02-17T08:25:44.521224Z",
          "iopub.status.idle": "2022-02-17T08:25:45.394177Z",
          "shell.execute_reply": "2022-02-17T08:25:45.393518Z",
          "shell.execute_reply.started": "2022-02-17T08:25:44.521470Z"
        },
        "id": "lVAnQYdD5LSr",
        "outputId": "55f91a45-0b3a-4d61-f85a-6a99b36d5bc8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 102/102 [00:21<00:00,  4.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed 54612 tokens with 5942 phrases; found: 5613 phrases; correct: 4185.\n",
            "accuracy:  69.27%; (non-O)\n",
            "accuracy:  94.61%; precision:  74.56%; recall:  70.43%; FB1:  72.44\n",
            "              LOC: precision:  87.82%; recall:  83.18%; FB1:  85.43  1740\n",
            "             MISC: precision:  78.91%; recall:  73.86%; FB1:  76.30  863\n",
            "              ORG: precision:  66.11%; recall:  65.18%; FB1:  65.64  1322\n",
            "              PER: precision:  65.28%; recall:  59.83%; FB1:  62.44  1688\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(74.55905932656334, 70.43083136990911, 72.43617481609694)"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use the conlleval script to measure the entity-level f1\n",
        "pred_tags = []\n",
        "for tags in predict(model, val_dataloader, device):\n",
        "    pred_tags.extend(tags)\n",
        "    pred_tags.append('O')\n",
        "    \n",
        "true_tags = []\n",
        "for tags in val_raw['tags']:\n",
        "    true_tags.extend(tags.strip().split())\n",
        "    true_tags.append('O')\n",
        "\n",
        "evaluate(true_tags, pred_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztKqd9J15LSr"
      },
      "source": [
        "Example output from the above codeblock. We will take the overall test F1 score (69.24 in this example) and grade accordingly.\n",
        "```\n",
        "processed 54612 tokens with 5942 phrases; found: 5554 phrases; correct: 3980.\n",
        "accuracy:  65.78%; (non-O)\n",
        "accuracy:  93.88%; precision:  71.66%; recall:  66.98%; FB1:  69.24\n",
        "              LOC: precision:  84.58%; recall:  77.03%; FB1:  80.63  1673\n",
        "             MISC: precision:  77.31%; recall:  71.69%; FB1:  74.40  855\n",
        "              ORG: precision:  58.71%; recall:  63.83%; FB1:  61.16  1458\n",
        "              PER: precision:  66.84%; recall:  56.89%; FB1:  61.47  1568\n",
        "(71.66006481814908, 66.98081454055873, 69.24147529575504)\n",
        "```\n",
        "If the codeblock above errors out, check your implementation of the `predict` function. It should return a nested list of lists, each containing predicted tags in their IOB string forms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-02-17T08:25:45.395822Z",
          "iopub.status.busy": "2022-02-17T08:25:45.395550Z",
          "iopub.status.idle": "2022-02-17T08:25:46.111937Z",
          "shell.execute_reply": "2022-02-17T08:25:46.111234Z",
          "shell.execute_reply.started": "2022-02-17T08:25:45.395787Z"
        },
        "id": "dVt102qy5LSs",
        "outputId": "5cfae824-2c8d-4458-bfe2-5bf9d11369b8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 108/108 [00:24<00:00,  4.34it/s]\n"
          ]
        }
      ],
      "source": [
        "# YOU SHOULD NOT CHANGE THIS CODEBLOCK\n",
        "# make prediction on the test set and save to submission.txt\n",
        "preds = predict(model, test_dataloader, device)\n",
        "with open(\"submission.txt\", \"w\") as f:\n",
        "    for tags in preds:\n",
        "        f.write(\" \".join(tags) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
